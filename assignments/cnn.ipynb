{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Assignment 1\n",
    "\n",
    "#### Part II: Convolutional Neural Networks\n",
    "\n",
    "***\n",
    "\n",
    "Please see the description of the assignment in the README file (section 2) <br>\n",
    "**Guide notebook**: [material/cnns_pytorch.ipynb](material/cnns_pytorch.ipynb)\n",
    "\n",
    "Table of contents:\n",
    "1. Activate GPU\n",
    "2. Load data\n",
    "3. Inspect data\n",
    "4. Convolutional neural network (**Where you will implement the CNN**)\n",
    "5. Training hyperparameters (**Where you will add training parameters**)\n",
    "6. Training\n",
    "7. Plot loss and accuracy\n",
    "8. Evaluate\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in relevant libraries, and alias where appropriate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision  # noqa\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F  # noqa\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a bit of a hack in case your IDE wants to run the notebook from /`assignment/` and not the project root folder `/ma1`. We need the working directory to be `/ma1` for local imports to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: c:\\Users\\Alexandru Plecan\\Desktop\\Bewerbung\\Studium\\BWL\\Master\\AIML25\\ma1\n"
     ]
    }
   ],
   "source": [
    "# Ensure the working directory is set to the \"ma1\" folder.\n",
    "while Path.cwd().name != \"ma1\" and \"ma1\" in str(Path.cwd()):\n",
    "    os.chdir(\"..\")  # Move up one directory\n",
    "print(f\"Working directory set to: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are local files imported into this notebook. This is one of the advantages of not using an online notebook like Google Colab or stand-alone notebooks like Jupyter: We can import local files and use them in our code, thus making it easier to manage our code and create a more modular structure.\n",
    "\n",
    "In `/src`, we have the following files:\n",
    "- `utils.py`: Contains utility functions (e.g., setting our device to GPU if available)\n",
    "- `data.py`: Contains functions to load data, train/validation split, and data augmentation\n",
    "- `training.py`: Contains a single but very important function: Our training loop.\n",
    "- `evaluation.py`: Contains functions to evaluate our model and produce a classification report\n",
    "- `visualization.py`: Contains functions to visualize our data and model performance\n",
    "\n",
    "You are encouraged to look at these files to understand how they work. Particularly, the training and evaluation files are important to understand how we train our model and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local files\n",
    "from src.utils import get_device\n",
    "from src.data import load_torch_data, to_dataloader, train_val_split\n",
    "from src.training import fit\n",
    "from src.evaluation import evaluate\n",
    "from src.visualize import plot_training_history, plot_probabilities, show_cifar_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Activate GPU\n",
    "If available. Note that this is not necessary, but it will speed up your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pytorch version (2.5.1) with backend = cpu\n"
     ]
    }
   ],
   "source": [
    "device = get_device()  # will default to 'cpu' if gpu is not available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1fe5373c4f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64  # number of samples per batch\n",
    "\n",
    "torch.manual_seed(42)  # Set a random seed to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'len(train_val)=50000, len(test)=10000'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use transforms.compose method to reformat images for modeling,\n",
    "# and save to variable preprocessing_stepss for later use.\n",
    "# Think of this like sci-kit learn's pipeline method.\n",
    "preprocessing_steps = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((32,32)),             # Cifar-10 images are 32x32\n",
    "        transforms.RandomCrop(32, padding=4),   # Data augmentation step: Randomly crop the image to 32x32\n",
    "        transforms.RandomHorizontalFlip(),      # Data augmentation step: Randomly flip image horizontally\n",
    "        transforms.ToTensor(),                  # Convert the image to a pytorch tensor\n",
    "        transforms.Normalize(                   # Normalize the image (i.e. scale the image to have a mean of 0 and a standard deviation of 1)\n",
    "            mean=[0.4914, 0.4822, 0.4465],      # FYI: The mean of the CIFAR10 dataset for your convenience\n",
    "            std=[0.2023, 0.1994, 0.2010]        # FYI the standard deviation of the CIFAR10 dataset\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load the training/validation data\n",
    "train_val = load_torch_data(\n",
    "    dataset=\"CIFAR10\",\n",
    "    root = 'data',                     # The root directory where the dataset will be stored\n",
    "    download = True,                   # If the dataset is not found at root, it will be downloaded\n",
    "    train = True,                      # The train dataset (as opposed to the test dataset)\n",
    "    transform = preprocessing_steps  # transformations to be applied to the dataset (see steps above)\n",
    ")\n",
    "\n",
    "# load the testing data\n",
    "test = load_torch_data(\n",
    "    dataset = \"CIFAR10\",\n",
    "    root = 'data',\n",
    "    download = True,\n",
    "    train = False,\n",
    "    transform = preprocessing_steps\n",
    ")\n",
    "\n",
    "f\"{len(train_val)=}, {len(test)=}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'len(val)=10000, len(train)=40000'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split training data in training and validation (just like train_test_split in sklearn)\n",
    "train, val = train_val_split(train_val, val_ratio=0.2, seed=42)\n",
    "\n",
    "f\"{len(val)=}, {len(train)=}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloders for easy batch loading during training\n",
    "train_loader = to_dataloader(train, batch_size = batch_size, shuffle = True)\n",
    "val_loader = to_dataloader(val, batch_size = batch_size, shuffle = False)\n",
    "test_loader = to_dataloader(test, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset CIFAR10\n",
       "     Number of datapoints: 50000\n",
       "     Root location: data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)\n",
       "                RandomCrop(size=(32, 32), padding=4)\n",
       "                RandomHorizontalFlip(p=0.5)\n",
       "                ToTensor()\n",
       "                Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])\n",
       "            ),\n",
       " Dataset CIFAR10\n",
       "     Number of datapoints: 10000\n",
       "     Root location: data\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)\n",
       "                RandomCrop(size=(32, 32), padding=4)\n",
       "                RandomHorizontalFlip(p=0.5)\n",
       "                ToTensor()\n",
       "                Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])\n",
       "            ))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Pick a random example from the training set\n",
    "classes = train.dataset.classes\n",
    "selection = random.randrange(len(train)-1)\n",
    "image, label = train[selection]\n",
    "\n",
    "show_cifar_img(image, label, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        ######################\n",
    "        # Convolutional Layers\n",
    "        ######################\n",
    "        # First convolution: Takes the 3 input channels (RGB) and produces 32 feature maps.\n",
    "        # Using kernel_size=3, stride=1, padding=1 keeps the spatial dimensions unchanged (32×32).\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # Batch normalization helps to stabilize learning and speed up convergence.\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        # Max pooling reduces the spatial size by a factor of 2 (from 32×32 to 16×16).\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second convolution: Takes the 32 channels and produces 64 feature maps.\n",
    "        # Again, kernel_size=3, stride=1, padding=1 keeps the feature map size the same before pooling.\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        # Second max pooling reduces the size from 16×16 to 8×8.\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        ######################\n",
    "        # Fully Connected Layers\n",
    "        ######################\n",
    "        # After two rounds of pooling, the 32×32 input becomes 8×8 spatially.\n",
    "        # With 64 feature maps, the flattened vector has 64 * 8 * 8 = 4096 features.\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Fully connected layer that maps the 4096 features to 1024 neurons.\n",
    "        self.fc1 = nn.Linear(in_features=64 * 8 * 8, out_features=1024)\n",
    "        # Dropout helps prevent overfitting by randomly zeroing some activations.\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        # Final fully connected layer outputs logits for 10 classes (CIFAR-10).\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=10)\n",
    "        # ReLU activation function (used multiple times in the network)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional block 1: conv -> batch norm -> ReLU -> pool\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Convolutional block 2: conv -> batch norm -> ReLU -> pool\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Flatten the output tensor to feed into the fully connected layers\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Fully connected block: fc -> ReLU -> dropout -> fc\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant variables for the ML task\n",
    "LEARNING_RATE = 1e-3 # TODO: Set the learning rate\n",
    "WEIGHT_DECAY = 1e-4 # TODO: Set the weight decay (i.e. L2 regularization)\n",
    "NUM_EPOCHS = 20 # TODO: Set the number of epochs to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device) # FYI; Sends the model to the GPU if set earlier\n",
    "\n",
    "# Set Loss function with criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set optimizer with optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# don't flatten input tensors to 1D before passing to model. Convolutonal layers expect nD.\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alexandru Plecan\\Desktop\\Bewerbung\\Studium\\BWL\\Master\\AIML25\\ma1\\src\\training.py:84\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, train_loader, val_loader, device, optimizer, criterion, num_epochs, flatten)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Backward pass: Compute gradients\u001b[39;00m\n\u001b[0;32m     83\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear previous gradients\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute new gradients\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Update model parameters\u001b[39;00m\n\u001b[0;32m     87\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Alexandru Plecan\\anaconda3\\envs\\aiml25-ma1\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alexandru Plecan\\anaconda3\\envs\\aiml25-ma1\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alexandru Plecan\\anaconda3\\envs\\aiml25-ma1\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, history = fit(\n",
    "    model = model,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    optimizer = optimizer,\n",
    "    criterion = criterion,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    device = device,\n",
    "    flatten = False  # don't flatten input tensors to 1D before passing to model. Convolutonal layers expect nD.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Plot loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on training data, validation data and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    model = model,\n",
    "    data_loader = train_loader,  # evaluate on training data\n",
    "    device = device,\n",
    "    criterion = criterion,\n",
    "    flatten = False  # don't flatten input tensors to 1D. Convolutonal layers expect nD.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single image and its label from the test dataset\n",
    "index = np.random.randint(len(test))  # Randomly select an index\n",
    "image, label = test[index]\n",
    "\n",
    "# Prepare the image for prediction\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    input_image = image.unsqueeze(0).to(device)                     # Add batch dimension and move to device\n",
    "    outputs = model(input_image)                                    # Get model predictions\n",
    "    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)  # Apply softmax to get probabilities\n",
    "\n",
    "plot_probabilities(\n",
    "    image = image,\n",
    "    label = label,\n",
    "    probabilities = probabilities, \n",
    "    classes = test.classes,\n",
    "    n = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    model = model,\n",
    "    data_loader = val_loader,  # evaluate on validation data\n",
    "    device = device,\n",
    "    criterion = criterion,\n",
    "    flatten = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    model = model,\n",
    "    data_loader = test_loader,  # evaluate on testing data\n",
    "    device = device,\n",
    "    criterion = criterion,\n",
    "    flatten = False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
